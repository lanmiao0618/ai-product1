'''
åŸºäºå†å²èŠå¤©è®°å½•çš„å¯¹è¯æ¨¡å‹
1ã€å¤§æ¨¡å‹å¯¹è±¡
2ã€æç¤ºè¯å·¥ç¨‹å¯¹è±¡
3ã€è®°å¿†æ¨¡å—å¯¹è±¡
4ã€chainé“¾å¯¹è±¡
'''
# åˆ¶ä½œä¸€ä¸ªèŠå¤©ç•Œé¢
# è§£å†³èŠå¤©ç•Œé¢ä¸èƒ½æ¸²æŸ“ä»¥å¾€æ—§å¯¹è¯ä¿¡æ¯
# streamlitæ¯æ¬¡è¾“å…¥æ¡†å‘é€å®Œæˆæ•°æ®ä¹‹åï¼Œé¡µé¢éƒ½ä¼šé‡æ–°åŠ è½½
# åªè¦å½“streamlité‡æ–°åŠ è½½çš„æ—¶å€™ï¼Œä¿è¯èŠå¤©è®°å½•ä¸è¢«æ¸…ç©ºï¼Œä¿¡æ¯
import streamlit as st
# langchainè°ƒç”¨å¤§æ¨¡å‹ï¼Œå¯¼å…¥langchainçš„ä»£ç  å¤§æ¨¡å‹å¯¹è±¡
from langchain_openai import ChatOpenAI
# å¼•å…¥ä¸€ä¸ªæç¤ºè¯å¯¹è±¡ langchainä¸­æœ‰å¾ˆå¤šæç¤ºè¯å¯¹è±¡ï¼Œåªç”¨ä¸€ä¸ªç®€å•çš„å¯¹è±¡PromptTemplate
from langchain.prompts import PromptTemplate
# å¼•å…¥ä¸€ä¸ªè®°å¿†æ¨¡å—å¯¹è±¡ï¼Œè®°å¿†æ¨¡å—ä¹Ÿæ˜¯å¾ˆå¤šç§
from langchain.memory import ConversationBufferMemory
# å¼•å…¥ä¸€ä¸ªlangchainçš„é“¾å¯¹è±¡
from langchain.chains import LLMChain
# æ„å»ºä¸€ä¸ªå¤§æ¨¡å‹ --æ™ºè°±AIå…¬å¸æä¾›çš„å¤§æ¨¡å‹
model = ChatOpenAI(
    temperature=0.8,  # æ¸©åº¦
    model="glm-4-plus",  # å¤§æ¨¡å‹çš„åå­—
    base_url="https://open.bigmodel.cn/api/paas/v4/",  # å¤§æ¨¡å‹çš„åœ°å€
    api_key="b4726e42f278d5b63e8cbd7400c87a97.NOuIfpwmouyKKw5R"  # è´¦å·ä¿¡æ¯
)
# åˆ›å»ºè®°å¿†æ¨¡å—å¯¹è±¡ è®°å¿†æ¨¡å—éœ€è¦ç»“åˆæç¤ºè¯æ¨¡å—ä½¿ç”¨ã€‚è®°å¿†æ¨¡å—ä¿å­˜çš„æ•°æ®å½“ä½œæç¤ºè¯ä¿¡æ¯ä¼ é€’ç»™å¤§æ¨¡å‹å³å¯
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="history")
# åˆ›å»ºæç¤ºè¯å¯¹è±¡
prompt = PromptTemplate.from_template("ä½ å«ç‰ç’ƒï¼Œä½ ç°åœ¨æ‰®æ¼”çš„æ˜¯ä¸€ä¸ªå¥³æœ‹å‹çš„è§’è‰²ï¼Œä½ ç°åœ¨è¦å’Œä½ çš„ç”·æœ‹å‹å¯¹è¯ï¼Œä½ ç”·æœ‹å‹çš„è¯æ˜¯{input}ï¼Œä½ éœ€è¦å¯¹ä½ ç”·æœ‹å‹çš„è¯ä½œå‡ºå›åº”ï¼Œè€Œä¸”åªåšå›åº”ï¼Œä½ å’Œä½ ç”·æœ‹å‹çš„å†å²å¯¹è¯ä¸º{history}")
# ä½¿ç”¨langchainé“¾å…³è”å¤§æ¨¡å‹å’Œæç¤ºè¯å¯¹è±¡
chain = LLMChain(
    llm=model,
    prompt=prompt,
    memory=st.session_state.memory
)
st.title("çµç®€ğŸŸ")
# æ„å»ºä¸€ä¸ªç¼“å­˜ï¼Œç”¨æ¥ä¿å­˜èŠå¤©è®°å½•
if "cache" not in st.session_state:
    st.session_state.cache = []
else:
    # éœ€è¦ä»ç¼“å­˜ä¸­è·å–å¯¹è¯ä¿¡æ¯åœ¨ç•Œé¢ä¸Šæ¸²æŸ“ ç¼“å­˜ä¸¤å—å†…å®¹ è§’è‰² è§’è‰²çš„æ¶ˆæ¯
    for message in st.session_state.cache:
        with st.chat_message(message['role']):
            st.write(message["content"])

# åˆ›å»ºä¸€ä¸ªèŠå¤©è¾“å…¥æ¡†
problem = st.chat_input("ä½ çš„å°çµæ­£åœ¨ç­‰å¾…ä½ çš„å›åº”")
# åˆ¤æ–­æ˜¯ç”¨æ¥ç¡®å®šç”¨æˆ·æœ‰æ²¡æœ‰è¾“å…¥é—®é¢˜ å¦‚æœè¾“å…¥é—®é¢˜
if problem:
    # 1ã€å°†ç”¨æˆ·çš„é—®é¢˜è¾“å‡ºåˆ°ç•Œé¢ä¸Šï¼Œä»¥ç”¨æˆ·çš„è§’è‰²è¾“å‡º
    with st.chat_message("user"):
        st.write(problem)
        st.session_state.cache.append({"role": "user", "content": problem})
    # 2ã€è°ƒç”¨é“¾å¯¹è±¡å›ç­”é—®é¢˜
    result = chain.invoke({"input":problem})
    # 3ã€å°†å¤§æ¨¡å‹å›ç­”çš„é—®é¢˜è¾“å‡ºåˆ°ç•Œé¢ä¸Š
    with st.chat_message("assistant"):
        st.write(result['text'])
        st.session_state.cache.append({"role": "assistant", "content": result['text']})
